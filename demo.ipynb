{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.3-py3-none-win_amd64.whl (95.2 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from xgboost) (1.5.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from xgboost) (1.19.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.3\n",
      "Collecting catboost\n",
      "  Downloading catboost-0.24.4-cp36-none-win_amd64.whl (65.4 MB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from catboost) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from catboost) (1.19.1)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from catboost) (3.3.3)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.16-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from catboost) (1.5.4)\n",
      "Requirement already satisfied: plotly in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from catboost) (4.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from pandas>=0.24.0->catboost) (2020.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from matplotlib->catboost) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from plotly->catboost) (1.3.3)\n",
      "Installing collected packages: graphviz, catboost\n",
      "Successfully installed catboost-0.24.4 graphviz-0.16\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.1.1-py2.py3-none-win_amd64.whl (754 kB)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from lightgbm) (0.35.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from lightgbm) (1.19.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from lightgbm) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dell\\anaconda3\\envs\\hoang\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.17.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install catboost\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-79fa71095500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hoang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hoang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hoang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hoang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\hoang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 550068 entries, 0 to 550067\n",
      "Data columns (total 12 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   User_ID                     550068 non-null  int64  \n",
      " 1   Product_ID                  550068 non-null  object \n",
      " 2   Gender                      550068 non-null  object \n",
      " 3   Age                         550068 non-null  object \n",
      " 4   Occupation                  550068 non-null  int64  \n",
      " 5   City_Category               550068 non-null  object \n",
      " 6   Stay_In_Current_City_Years  550068 non-null  object \n",
      " 7   Marital_Status              550068 non-null  int64  \n",
      " 8   Product_Category_1          550068 non-null  int64  \n",
      " 9   Product_Category_2          376430 non-null  float64\n",
      " 10  Product_Category_3          166821 non-null  float64\n",
      " 11  Purchase                    550068 non-null  int64  \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 50.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_train_test():\n",
    "    data = pd.read_csv('train.csv')\n",
    "    \n",
    "    # shoulde be categorical variables\n",
    "    cols = ['User_ID','Product_ID','Gender','Age',\n",
    "           'Occupation','City_Category','Stay_In_Current_City_Years','Marital_Status',\n",
    "           'Product_Category_1','Product_Category_2','Product_Category_3']\n",
    "    \n",
    "    for col in cols:\n",
    "        data[col] = data[col].astype('str')\n",
    "        \n",
    "    X = data[cols].copy()\n",
    "    y = data['Purchase'].copy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 10)\n",
    "    \n",
    "    data_train = pd.concat([X_train,y_train], axis = 1)\n",
    "    data_test = pd.concat([X_test,y_test], axis = 1)\n",
    "    \n",
    "    return [data_train, data_test]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_01: gender median purchase\n",
    "def get_feat_01(data_train):\n",
    "    df_tmp = data_train.groupby('Gender')['Purchase'].median().reset_index()\n",
    "    df_tmp.columns = ['Gender','feat_01']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_02: Gender std purchase\n",
    "def get_feat_02(data_train):\n",
    "    df_tmp = data_train.groupby('Gender')['Purchase'].std().reset_index()\n",
    "    df_tmp.columns = ['Gender','feat_02']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_03: Age median purchase\n",
    "def get_feat_03(data_train):\n",
    "    df_tmp = data_train.groupby('Age')['Purchase'].median().reset_index()\n",
    "    df_tmp.columns = ['Age','feat_03']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_04: Age std purchase\n",
    "def get_feat_04(data_train):\n",
    "    df_tmp = data_train.groupby('Age')['Purchase'].std().reset_index()\n",
    "    df_tmp.columns = ['Age','feat_04']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_05: Occupation median purchase\n",
    "def get_feat_05(data_train):\n",
    "    df_tmp = data_train.groupby('Occupation')['Purchase'].median().reset_index()\n",
    "    df_tmp.columns = ['Occupation','feat_05']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_06: Occupation std purchase\n",
    "def get_feat_06(data_train):\n",
    "    df_tmp = data_train.groupby('Occupation')['Purchase'].std().reset_index()\n",
    "    df_tmp.columns = ['Occupation','feat_06']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_07: Product_Category_1 median purchase\n",
    "def get_feat_07(data_train):\n",
    "    df_tmp = data_train.groupby('Product_Category_1')['Purchase'].median().reset_index()\n",
    "    df_tmp.columns = ['Product_Category_1','feat_07']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_08: Product_Category_1 std purchase\n",
    "def get_feat_08(data_train):\n",
    "    df_tmp = data_train.groupby('Product_Category_1')['Purchase'].std().reset_index()\n",
    "    df_tmp.columns = ['Product_Category_1','feat_08']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_09: product_id -- nunique user_id\n",
    "def get_feat_09(data_train):\n",
    "    df_tmp = data_train.groupby('Product_ID')['User_ID'].nunique().reset_index()\n",
    "    df_tmp.columns = ['Product_ID','feat_09']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_10: product_id - median purchase\n",
    "def get_feat_10(data_train):\n",
    "    df_tmp = data_train.groupby('Product_ID')['Purchase'].median().reset_index()\n",
    "    df_tmp.columns = ['Product_ID','feat_10']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_11: product_id - std purchase\n",
    "def get_feat_11(data_train):\n",
    "    df_tmp = data_train.groupby('Product_ID')['Purchase'].std().reset_index()\n",
    "    df_tmp.columns = ['Product_ID','feat_11']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_12:  user_id - nunique product_id\n",
    "def get_feat_12(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Product_ID'].nunique().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_12']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_13: User_iID median purchase\n",
    "def get_feat_13(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Purchase'].median().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_13']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_14: User_ID std purchase\n",
    "def get_feat_14(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Purchase'].std().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_14']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_15: User_ID count Product_Category_1\n",
    "def get_feat_15(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Product_Category_1'].count().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_15']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_16: User_ID nunique Product_Category_1\n",
    "def get_feat_16(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Product_Category_1'].nunique().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_16']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_17:User_ID count Product_Category_1\n",
    "def get_feat_17(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Product_Category_2'].count().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_17']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_18: User_ID nunique Product_Category_1\n",
    "def get_feat_18(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Product_Category_2'].nunique().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_18']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_19: User_ID count Product_Category_3\n",
    "def get_feat_19(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Product_Category_3'].count().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_19']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_20: User_ID nunique Product_CAtegory_3\n",
    "def get_feat_20(data_train):\n",
    "    df_tmp = data_train.groupby('User_ID')['Product_Category_3'].nunique().reset_index()\n",
    "    df_tmp.columns = ['User_ID','feat_20']\n",
    "    return df_tmp\n",
    "\n",
    "# feat_21\n",
    "def get_feat_21(data_train):\n",
    "    df_feat_10 = get_feat_10(data_train)\n",
    "    df_tmp = pd.merge(data_train,df_feat_10, on = 'Product_ID')\n",
    "#     df_tmp = pd.merge(data_train[['Product_ID','Purchase']],df_feat_10, on = 'Product_ID')\n",
    "    df_tmp['feat_21'] = (df_tmp['Purchase'] - df_tmp['feat_10'])/df_tmp['feat_10']\n",
    "    \n",
    "    return df_tmp[['User_ID','Product_ID','feat_21']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_engineering(data_train,data_test):\n",
    "    data = data_train.copy()\n",
    "    \n",
    "    df_feat_01 = get_feat_01(data)\n",
    "    df_feat_02 = get_feat_02(data)\n",
    "    df_feat_03 = get_feat_03(data)\n",
    "    df_feat_04 = get_feat_04(data)\n",
    "    df_feat_05 = get_feat_05(data)\n",
    "    df_feat_06 = get_feat_06(data)\n",
    "    df_feat_07 = get_feat_07(data)\n",
    "    df_feat_08 = get_feat_08(data)\n",
    "    df_feat_09 = get_feat_09(data)\n",
    "    df_feat_10 = get_feat_10(data)\n",
    "    df_feat_11 = get_feat_11(data)\n",
    "    df_feat_12 = get_feat_12(data)\n",
    "    df_feat_13 = get_feat_13(data)\n",
    "    df_feat_14 = get_feat_14(data)\n",
    "    df_feat_15 = get_feat_15(data)\n",
    "    df_feat_16 = get_feat_16(data)\n",
    "    df_feat_17 = get_feat_17(data)\n",
    "    df_feat_18 = get_feat_18(data)\n",
    "    df_feat_19 = get_feat_19(data)\n",
    "    df_feat_20 = get_feat_20(data)\n",
    "#     df_feat_21 = get_feat_21(data)\n",
    "    \n",
    "    data = pd.merge(data,df_feat_01, on = 'Gender', how = 'left')\n",
    "    data = pd.merge(data,df_feat_02, on = 'Gender', how = 'left')\n",
    "    data = pd.merge(data,df_feat_03, on = 'Age', how = 'left')\n",
    "    data = pd.merge(data,df_feat_04, on = 'Age', how = 'left')\n",
    "    data = pd.merge(data,df_feat_05, on = 'Occupation', how = 'left')\n",
    "    data = pd.merge(data,df_feat_06, on = 'Occupation', how = 'left')\n",
    "    data = pd.merge(data,df_feat_07, on = 'Product_Category_1', how = 'left')\n",
    "    data = pd.merge(data,df_feat_08, on = 'Product_Category_1', how = 'left')\n",
    "    data = pd.merge(data,df_feat_09, on = 'Product_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_10, on = 'Product_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_11, on = 'Product_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_12, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_13, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_14, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_15, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_16, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_17, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_18, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_19, on = 'User_ID', how = 'left')\n",
    "    data = pd.merge(data,df_feat_20, on = 'User_ID', how = 'left')\n",
    "#     data = pd.merge(data,df_feat_21, on = ['User_ID','Product_ID'], how = 'left')\n",
    "    \n",
    "    user_cols = ['User_ID','Gender','Age','Occupation',\n",
    "                'City_Category','Stay_In_Current_City_Years','Marital_Status',\n",
    "                'feat_01','feat_02','feat_03','feat_04',\n",
    "                'feat_05','feat_06','feat_12','feat_13',\n",
    "                'feat_14','feat_15','feat_16','feat_17',\n",
    "                'feat_18','feat_19','feat_20']\n",
    "    \n",
    "    prod_cols = ['Product_ID','Product_Category_1','Product_Category_2','Product_Category_3',\n",
    "                'feat_07','feat_08','feat_09','feat_10','feat_11']\n",
    "    \n",
    "    df_user = data[user_cols].drop_duplicates()\n",
    "    df_prod = data[prod_cols].drop_duplicates()\n",
    "    \n",
    "    data_test_00 = data_test[['User_ID','Product_ID','Purchase']]\n",
    "    data_test_01 = pd.merge(data_test_00,df_user, on = 'User_ID', how = 'left')\n",
    "    data_test_02 = pd.merge(data_test_01,df_prod, on = 'Product_ID', how = 'left')\n",
    "#     data_test_02 = pd.merge(data_test_02,data[['User_ID','Product_ID','feat_21']], on = ['User_ID','Product_ID'], how = 'left')\n",
    "    \n",
    "    X = data.drop(columns = ['Purchase'])\n",
    "    y = data['Purchase']\n",
    "    \n",
    "    selected_cols = list(data.columns)\n",
    "    \n",
    "    data = pd.concat([X,y], axis = 1)\n",
    "    \n",
    "    data_test_02 = data_test_02[selected_cols]\n",
    "    \n",
    "    data['flag'] = 'train'\n",
    "    data_test_02['flag'] = 'test'\n",
    "    \n",
    "    \n",
    "    X_new = pd.concat([data,data_test_02])\n",
    "    \n",
    "    \n",
    "    categorical_variables = ['Age','Stay_In_Current_City_Years','Gender',\n",
    "                            'Occupation','City_Category','Marital_Status',\n",
    "                            'Product_Category_1','Product_Category_2','Product_Category_3']\n",
    "    \n",
    "    for col in categorical_variables:\n",
    "        X_new = pd.get_dummies(X_new, prefix = [col], columns = [col])\n",
    "        \n",
    "    data_train_new = X_new[X_new.flag == 'train']\n",
    "    data_test_new = X_new[X_new.flag == 'test']\n",
    "    \n",
    "    \n",
    "    data_train_new = data_train_new.drop(columns = ['flag'])\n",
    "    data_test_new = data_test_new.drop(columns = ['flag'])\n",
    "    \n",
    "#     numerical_features = ['feat_09','feat_10','feat_12','feat_13',\n",
    "#                          'feat_15','feat_16','feat_17','feat_18','feat_19','feat_20','Purchase']\n",
    "#     numerical_features = ['Purchase']\n",
    "    \n",
    "#     outlier_indices = detect_outliers(data_train_new,1,numerical_features)\n",
    "    \n",
    "#     data_train_new = data_train_new.drop(outlier_indices, axis = 0).reset_index(drop = True)\n",
    "    \n",
    "    return [data_train_new,data_test_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_fixed_xgb():\n",
    "    my_params = {\n",
    "        'scale_pos_weight': 1,\n",
    "        'learning_rate': 0.01,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'n_estimators': 400,\n",
    "        'reg_alpha': 0.5,\n",
    "        'max_depth': 15,\n",
    "        'gamma': 1\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(\n",
    "        scale_pos_weight = my_params['scale_pos_weight'],\n",
    "        learning_rate = my_params['learning_rate'],\n",
    "        colsample_bytree = my_params['colsample_bytree'],\n",
    "        subsample = my_params['subsample'],\n",
    "        objective = my_params['objective'],\n",
    "        eval_metric = my_params['eval_metric'],\n",
    "        n_estimators = my_params['n_estimators'],\n",
    "        reg_alpha = my_params['reg_alpha'],\n",
    "        max_depth = my_params['max_depth'],\n",
    "        gamma = my_params['gamma']\n",
    "    )\n",
    "    \n",
    "    return xgb\n",
    "\n",
    "# def initialize_fixed_xgb():\n",
    "#     my_params = {\n",
    "#         'scale_pos_weight': 1,\n",
    "#         'learning_rate': 0.01,\n",
    "#         'colsample_bytree': 0.6,\n",
    "#         'subsample': 0.8,\n",
    "#         'objective': 'reg:squarederror',\n",
    "#         'eval_metric': 'rmse',\n",
    "#         'n_estimators': 1200, # 1200\n",
    "#         'reg_alpha': 0.5,\n",
    "#         'max_depth': 15,\n",
    "#         'gamma': 1\n",
    "#     }\n",
    "    \n",
    "#     xgb = XGBRegressor(\n",
    "#         scale_pos_weight = my_params['scale_pos_weight'],\n",
    "#         learning_rate = my_params['learning_rate'],\n",
    "#         colsample_bytree = my_params['colsample_bytree'],\n",
    "#         subsample = my_params['subsample'],\n",
    "#         objective = my_params['objective'],\n",
    "#         eval_metric = my_params['eval_metric'],\n",
    "#         n_estimators = my_params['n_estimators'],\n",
    "#         reg_alpha = my_params['reg_alpha'],\n",
    "#         max_depth = my_params['max_depth'],\n",
    "#         gamma = my_params['gamma']\n",
    "#     )\n",
    "    \n",
    "#     return xgb\n",
    "\n",
    "def initialize_fixed_lgbm():\n",
    "    my_params = {\n",
    "        'n_estimators': 480, # 480\n",
    "        'learning_rate': 0.35, # 0.35\n",
    "        'max_depth': 15,\n",
    "        'num_leaves': 40,\n",
    "        'feature_fraction': 0.8,\n",
    "        'subsample': 0.2,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'metric': 'rmse',\n",
    "        'bagging_freq': 20,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'boosting': 'dart',\n",
    "        'num_boost_round': 300,\n",
    "        'early_stopping_rounds': 30,\n",
    "        'min_data_in_leaf': 22, # 20\n",
    "        'min_sum_hessian_in_leaf': 400 # 400\n",
    "    }\n",
    "    \n",
    "    lgbm = LGBMRegressor(\n",
    "        n_estimators = my_params['n_estimators'],\n",
    "        learning_rate = my_params['learning_rate'],\n",
    "        max_depth = my_params['max_depth'],\n",
    "        num_leaves = my_params['num_leaves'],\n",
    "        feature_fraction = my_params['feature_fraction'],\n",
    "        subsample = my_params['subsample'],\n",
    "        metric = my_params['metric'],\n",
    "        colsample_bytree = my_params['colsample_bytree'],\n",
    "        min_data_in_leaf = my_params['min_data_in_leaf'],\n",
    "        min_sum_hessian_in_leaf = my_params['min_sum_hessian_in_leaf']\n",
    "    )\n",
    "    \n",
    "    return lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train_new: data for train after doing feature engineering ---> scaling\n",
    "# data_test_new: data for test after doing feature engineering\n",
    "# model_name: mo hinh minh lua chon da dung, linear, random foresting, mainly use boosting: CatBoost, XGBoost, LightGBM\n",
    "def train_model(data_train_new,data_test_new,model_name):\n",
    "    # X_train, y_train, X_test, y_test\n",
    "    X_train = data_train_new.drop(columns = ['User_ID','Product_ID','Purchase'])\n",
    "    y_train = data_train_new['Purchase']\n",
    "    \n",
    "    X_test = data_test_new.drop(columns = ['User_ID','Product_ID','Purchase'])\n",
    "    y_test = data_test_new['Purchase']\n",
    "    \n",
    "    if model_name == 'cb':\n",
    "        print('Error')\n",
    "    elif model_name == 'xgb':\n",
    "        model = initialize_fixed_xgb()\n",
    "        model.fit(X_train,y_train)\n",
    "    elif model_name == 'lgbm':\n",
    "        model = initialize_fixed_lgbm()\n",
    "        model.fit(X_train,y_train)\n",
    "    else:\n",
    "        print('Error')\n",
    "        \n",
    "    y_preds_test = model.predict(X_test)\n",
    "    y_preds_train = model.predict(X_train)\n",
    "    \n",
    "    return [y_preds_train,y_preds_test]\n",
    "    \n",
    "        \n",
    "#     selected_cols = list(X_test.columns)\n",
    "#     for col in selected_cols:\n",
    "#         X_test[col] = X_test[col].fillna(X_test[col].median())\n",
    "        \n",
    "#     y_preds_test = model.predict(X_test)\n",
    "#     y_preds_train = model.predict(X_train)\n",
    "    \n",
    "#     return [y_preds_train,y_preds_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Loading data\n",
    "[data_train,data_test] = get_data_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: feature engineering\n",
    "[data_train_new,data_test_new]= get_feature_engineering(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.6 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=22\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=400, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=400\n"
     ]
    }
   ],
   "source": [
    "# # XGBoost\n",
    "# model_name = 'xgb'\n",
    "# [y_preds_train,y_preds_test] = train_model(data_train_new,data_test_new,model_name)\n",
    "# y_preds_train_xgb = y_preds_train.copy()\n",
    "# y_preds_test_xgb = y_preds_test.copy()\n",
    "\n",
    "# # # LightGBM\n",
    "model_name = 'lgbm'\n",
    "[y_preds_train,y_preds_test] = train_model(data_train_new,data_test_new,model_name)\n",
    "y_preds_train_lgbm = y_preds_train.copy()\n",
    "y_preds_test_lgbm = y_preds_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2475.6202533065666"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure rmse\n",
    "y_test = data_test_new['Purchase']\n",
    "# rmse_cb = np.sqrt(metrics.mean_squared_error(y_test,y_preds_test_cb))\n",
    "# rmse_cb\n",
    "# rmse_xgb = np.sqrt(metrics.mean_squared_error(y_test,y_preds_test_xgb))\n",
    "# rmse_xgb\n",
    "\n",
    "rmse_lgbm = np.sqrt(metrics.mean_squared_error(y_test,y_preds_test_lgbm))\n",
    "rmse_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2837.8107511248772 ---> 2902.307395138423 ---> 2610.4992918750586 ---> 2475.6202533065666 ---> 2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality checking ---> pass\n",
    "# feature engineering --> qua trinh tao ra cac feature moi tu feature goc ban dau da cho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
